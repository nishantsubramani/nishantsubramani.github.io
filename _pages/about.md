---
layout: about
title: about
permalink: /
subtitle: Model Interpretability Researcher ðŸ”Ž 

profile:
  align: left
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: #>
    #<p>555 your office number</p>
    #<p>123 your address street</p>
    #<p>Your City, State 12345</p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---
*Looking for research internships in summer 2025 focused on model interpretability; please reach out!*

I am a 2nd year PhD student ðŸŽ“ at [CMU](https://cmu.edu) in the [Language Technologies Institute (LTI)](https://lti.cs.cmu.edu/) advised by [Mona Diab](https://scholar.google.com/citations?user=-y6SIhQAAAAJ&hl=en). I have published at most of the major ML and NLP venues and was a part of the [BLOOM](https://bigscience.huggingface.co/blog/bloom) and [OLMo](https://allenai.org/olmo) open LLM efforts leading to two [best paper awards at ACL2024](https://2024.aclweb.org/program/best_papers/) and a [GeekWire innovation of the year award](https://www.geekwire.com/2024/ai-and-telecom-breakthroughs-dominate-innovation-of-the-year-category-for-2024-geekwire-awards/i). I spent this past summer as an intern on the [semantic machines team at Microsoft Research](https://www.microsoft.com/en-us/research/group/semantic-machines/) on [Ben Van Durme](https://www.cs.jhu.edu/~vandurme/) and [Jason Eisner's](https://www.cs.jhu.edu/~jason/) team.

My main research interest is in model interpretability ðŸ”Ž: the intersection of mechanistic and traditional interpretability. Specifically I am excited about understanding the internals of models and identifying steerable regions in the latent spaces of models. I wrote some of the first papers on steering vectors for [LSTM-based models at NeurIPS 2019](https://arxiv.org/abs/1907.04944) and [Transformer-based ones at ACL 2022](https://aclanthology.org/2022.findings-acl.48/), which have recently gained popularity. Understanding model internals is essential to help us build more responsible, controllable, trustworthy, and efficient NLP systems. If you're actively working in these directions, I'd love to chat and perhaps collaborate! Please reach out :smile: 

Before CMU, I spent nearly two and a half years as a predoctoral researcher on the AllenNLP team at [AI2](https://allenai.org/), where I worked with Matt Peters. Before that I spent two years in industry at startups as a research scientist working on NLP, vision, speech, and multimodal applications. I have had the opportunity to work closely with some amazing collaborators at other institutions including [Margaret Mitchell](https://www.m-mitchell.com/), [Sasha Luccioni](https://www.sashaluccioni.com/), [Vladlen Koltun](https://vladlen.info/), and [Doug Downey](https://users.cs.northwestern.edu/~ddowney/).  

